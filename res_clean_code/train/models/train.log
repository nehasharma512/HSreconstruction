2019-02-20 13:33:19 - Epoch [1], Iter[584], Time:500.138353348, learning rate : 0.000199819, Train Loss: 0.320907521 Test Loss: 0.243831417 
2019-02-20 13:41:28 - Epoch [2], Iter[1168], Time:488.658921957, learning rate : 0.000199638, Train Loss: 0.231016386 Test Loss: 0.234716619 
2019-02-20 13:49:37 - Epoch [3], Iter[1752], Time:488.606346369, learning rate : 0.000199458, Train Loss: 0.218038942 Test Loss: 0.230534506 
2019-02-20 13:57:45 - Epoch [4], Iter[2336], Time:488.613515615, learning rate : 0.000199277, Train Loss: 0.212742564 Test Loss: 0.230023914 
2019-02-20 14:05:54 - Epoch [5], Iter[2920], Time:488.560724974, learning rate : 0.000199096, Train Loss: 0.208701192 Test Loss: 0.225243994 
2019-02-20 14:14:02 - Epoch [6], Iter[3504], Time:488.582681894, learning rate : 0.000198915, Train Loss: 0.203577009 Test Loss: 0.219681675 
2019-02-20 14:22:11 - Epoch [7], Iter[4088], Time:488.524615765, learning rate : 0.000198735, Train Loss: 0.200656172 Test Loss: 0.214421837 
2019-02-20 14:30:19 - Epoch [8], Iter[4672], Time:488.533380270, learning rate : 0.000198554, Train Loss: 0.195560231 Test Loss: 0.207971406 
2019-02-20 14:38:28 - Epoch [9], Iter[5256], Time:488.518002033, learning rate : 0.000198374, Train Loss: 0.191145129 Test Loss: 0.217253202 
2019-02-20 14:46:36 - Epoch [10], Iter[5840], Time:488.437961102, learning rate : 0.000198193, Train Loss: 0.185659764 Test Loss: 0.213588718 
2019-02-20 14:54:45 - Epoch [11], Iter[6424], Time:488.455536127, learning rate : 0.000198013, Train Loss: 0.177544634 Test Loss: 0.208168518 
2019-02-20 15:02:53 - Epoch [12], Iter[7008], Time:488.648550272, learning rate : 0.000197832, Train Loss: 0.153249080 Test Loss: 0.192272525 
2019-02-20 15:11:02 - Epoch [13], Iter[7592], Time:488.710263014, learning rate : 0.000197652, Train Loss: 0.138579172 Test Loss: 0.183941221 
2019-02-20 15:19:11 - Epoch [14], Iter[8176], Time:488.726303101, learning rate : 0.000197472, Train Loss: 0.129934203 Test Loss: 0.166537275 
2019-02-20 15:27:19 - Epoch [15], Iter[8760], Time:488.495430946, learning rate : 0.000197292, Train Loss: 0.122982306 Test Loss: 0.157898866 
2019-02-20 15:35:28 - Epoch [16], Iter[9344], Time:488.586991549, learning rate : 0.000197111, Train Loss: 0.115217021 Test Loss: 0.160412600 
2019-02-20 15:43:37 - Epoch [17], Iter[9928], Time:488.575628519, learning rate : 0.000196931, Train Loss: 0.111261538 Test Loss: 0.151791764 
2019-02-20 15:51:45 - Epoch [18], Iter[10512], Time:488.677239180, learning rate : 0.000196751, Train Loss: 0.106449895 Test Loss: 0.142481348 
2019-02-20 15:59:54 - Epoch [19], Iter[11096], Time:488.565213680, learning rate : 0.000196571, Train Loss: 0.103782210 Test Loss: 0.143135372 
2019-02-20 16:08:02 - Epoch [20], Iter[11680], Time:488.588911533, learning rate : 0.000196391, Train Loss: 0.101202285 Test Loss: 0.141424562 
2019-02-20 16:16:11 - Epoch [21], Iter[12264], Time:488.541845322, learning rate : 0.000196212, Train Loss: 0.098131040 Test Loss: 0.137034714 
2019-02-20 16:24:19 - Epoch [22], Iter[12848], Time:488.489562273, learning rate : 0.000196032, Train Loss: 0.096744591 Test Loss: 0.142076765 
2019-02-20 16:32:28 - Epoch [23], Iter[13432], Time:488.549764872, learning rate : 0.000195852, Train Loss: 0.094045485 Test Loss: 0.128298376 
2019-02-20 16:40:37 - Epoch [24], Iter[14016], Time:488.596047640, learning rate : 0.000195672, Train Loss: 0.092475479 Test Loss: 0.133276123 
2019-02-20 16:48:45 - Epoch [25], Iter[14600], Time:488.496593952, learning rate : 0.000195493, Train Loss: 0.090229780 Test Loss: 0.131599310 
2019-02-20 16:56:54 - Epoch [26], Iter[15184], Time:488.610342026, learning rate : 0.000195313, Train Loss: 0.087752886 Test Loss: 0.131511253 
2019-02-20 17:05:02 - Epoch [27], Iter[15768], Time:488.526458740, learning rate : 0.000195133, Train Loss: 0.086764271 Test Loss: 0.130644638 
2019-02-20 17:13:11 - Epoch [28], Iter[16352], Time:488.559880495, learning rate : 0.000194954, Train Loss: 0.084059072 Test Loss: 0.132472919 
2019-02-20 17:21:19 - Epoch [29], Iter[16936], Time:488.476364851, learning rate : 0.000194775, Train Loss: 0.084245003 Test Loss: 0.132027000 
2019-02-20 17:29:28 - Epoch [30], Iter[17520], Time:488.519992113, learning rate : 0.000194595, Train Loss: 0.081954502 Test Loss: 0.123878877 
2019-02-20 17:37:36 - Epoch [31], Iter[18104], Time:488.512532473, learning rate : 0.000194416, Train Loss: 0.081684051 Test Loss: 0.129997381 
2019-02-20 17:45:45 - Epoch [32], Iter[18688], Time:488.527840853, learning rate : 0.000194237, Train Loss: 0.079432819 Test Loss: 0.129849547 
2019-02-20 17:53:53 - Epoch [33], Iter[19272], Time:488.540030956, learning rate : 0.000194057, Train Loss: 0.079502829 Test Loss: 0.125063213 
2019-02-20 18:02:02 - Epoch [34], Iter[19856], Time:488.555765629, learning rate : 0.000193878, Train Loss: 0.076779910 Test Loss: 0.124432612 
2019-02-20 18:10:11 - Epoch [35], Iter[20440], Time:488.668419600, learning rate : 0.000193699, Train Loss: 0.075645466 Test Loss: 0.118360273 
2019-02-20 18:18:19 - Epoch [36], Iter[21024], Time:488.451899767, learning rate : 0.000193520, Train Loss: 0.076273096 Test Loss: 0.129679481 
2019-02-20 18:26:28 - Epoch [37], Iter[21608], Time:488.469516516, learning rate : 0.000193341, Train Loss: 0.073992731 Test Loss: 0.127651287 
2019-02-20 18:34:36 - Epoch [38], Iter[22192], Time:488.500883341, learning rate : 0.000193162, Train Loss: 0.074229067 Test Loss: 0.120772981 
2019-02-20 18:42:45 - Epoch [39], Iter[22776], Time:488.585647583, learning rate : 0.000192983, Train Loss: 0.072357152 Test Loss: 0.121069855 
2019-02-20 18:50:53 - Epoch [40], Iter[23360], Time:488.531697750, learning rate : 0.000192804, Train Loss: 0.071723186 Test Loss: 0.127658088 
2019-02-20 18:59:02 - Epoch [41], Iter[23944], Time:488.496153355, learning rate : 0.000192626, Train Loss: 0.071675782 Test Loss: 0.119508466 
2019-02-20 19:07:10 - Epoch [42], Iter[24528], Time:488.473030567, learning rate : 0.000192447, Train Loss: 0.070234594 Test Loss: 0.122644238 
2019-02-20 19:15:19 - Epoch [43], Iter[25112], Time:488.593336105, learning rate : 0.000192268, Train Loss: 0.069285388 Test Loss: 0.115388071 
2019-02-20 19:23:27 - Epoch [44], Iter[25696], Time:488.526273012, learning rate : 0.000192090, Train Loss: 0.069343450 Test Loss: 0.119221622 
2019-02-20 19:31:36 - Epoch [45], Iter[26280], Time:488.552484274, learning rate : 0.000191911, Train Loss: 0.069664810 Test Loss: 0.119154895 
2019-02-20 19:39:44 - Epoch [46], Iter[26864], Time:488.548798323, learning rate : 0.000191733, Train Loss: 0.067976335 Test Loss: 0.117622754 
2019-02-20 19:47:53 - Epoch [47], Iter[27448], Time:488.490716457, learning rate : 0.000191554, Train Loss: 0.067775660 Test Loss: 0.116817725 
2019-02-20 19:56:01 - Epoch [48], Iter[28032], Time:488.475395679, learning rate : 0.000191376, Train Loss: 0.066796726 Test Loss: 0.115480875 
2019-02-20 20:04:10 - Epoch [49], Iter[28616], Time:488.601407766, learning rate : 0.000191198, Train Loss: 0.065329987 Test Loss: 0.122256591 
2019-02-20 20:12:19 - Epoch [50], Iter[29200], Time:488.622048855, learning rate : 0.000191019, Train Loss: 0.066239918 Test Loss: 0.113725618 
2019-02-20 20:20:27 - Epoch [51], Iter[29784], Time:488.531054735, learning rate : 0.000190841, Train Loss: 0.064270139 Test Loss: 0.127899174 
2019-02-20 20:28:36 - Epoch [52], Iter[30368], Time:488.473480940, learning rate : 0.000190663, Train Loss: 0.063986491 Test Loss: 0.119190139 
2019-02-20 20:36:44 - Epoch [53], Iter[30952], Time:488.549724817, learning rate : 0.000190485, Train Loss: 0.063864354 Test Loss: 0.122120671 
2019-02-20 20:44:53 - Epoch [54], Iter[31536], Time:488.525598049, learning rate : 0.000190307, Train Loss: 0.064673622 Test Loss: 0.115923360 
2019-02-20 20:53:01 - Epoch [55], Iter[32120], Time:488.550806046, learning rate : 0.000190129, Train Loss: 0.062683214 Test Loss: 0.115487666 
2019-02-20 21:01:10 - Epoch [56], Iter[32704], Time:488.450637341, learning rate : 0.000189951, Train Loss: 0.061744986 Test Loss: 0.117220289 
2019-02-20 21:09:18 - Epoch [57], Iter[33288], Time:488.518264771, learning rate : 0.000189773, Train Loss: 0.061345504 Test Loss: 0.116057305 
2019-02-20 21:17:27 - Epoch [58], Iter[33872], Time:488.451230049, learning rate : 0.000189595, Train Loss: 0.061020411 Test Loss: 0.115694716 
2019-02-20 21:25:35 - Epoch [59], Iter[34456], Time:488.546407938, learning rate : 0.000189417, Train Loss: 0.060897450 Test Loss: 0.113199657 
2019-02-20 21:33:44 - Epoch [60], Iter[35040], Time:488.514229774, learning rate : 0.000189240, Train Loss: 0.060662200 Test Loss: 0.118197407 
2019-02-20 21:41:52 - Epoch [61], Iter[35624], Time:488.450161934, learning rate : 0.000189062, Train Loss: 0.059988538 Test Loss: 0.114321880 
2019-02-20 21:50:01 - Epoch [62], Iter[36208], Time:488.646360159, learning rate : 0.000188884, Train Loss: 0.059052829 Test Loss: 0.112876843 
2019-02-20 21:58:09 - Epoch [63], Iter[36792], Time:488.539017200, learning rate : 0.000188707, Train Loss: 0.059035239 Test Loss: 0.113850058 
2019-02-20 22:06:18 - Epoch [64], Iter[37376], Time:488.585927248, learning rate : 0.000188529, Train Loss: 0.059075872 Test Loss: 0.109206366 
2019-02-20 22:14:26 - Epoch [65], Iter[37960], Time:488.414731741, learning rate : 0.000188352, Train Loss: 0.058054579 Test Loss: 0.118524064 
2019-02-20 22:22:35 - Epoch [66], Iter[38544], Time:488.457784653, learning rate : 0.000188175, Train Loss: 0.057224518 Test Loss: 0.114130539 
2019-02-20 22:30:43 - Epoch [67], Iter[39128], Time:488.452283382, learning rate : 0.000187997, Train Loss: 0.057818356 Test Loss: 0.123531547 
2019-02-20 22:38:52 - Epoch [68], Iter[39712], Time:488.451645851, learning rate : 0.000187820, Train Loss: 0.056762901 Test Loss: 0.115741199 
2019-02-20 22:47:00 - Epoch [69], Iter[40296], Time:488.475066662, learning rate : 0.000187643, Train Loss: 0.056778543 Test Loss: 0.109959274 
2019-02-20 22:55:09 - Epoch [70], Iter[40880], Time:488.485033751, learning rate : 0.000187466, Train Loss: 0.057825193 Test Loss: 0.117364356 
2019-02-20 23:03:17 - Epoch [71], Iter[41464], Time:488.531230927, learning rate : 0.000187288, Train Loss: 0.057372697 Test Loss: 0.112905671 
2019-02-20 23:11:26 - Epoch [72], Iter[42048], Time:488.512162447, learning rate : 0.000187111, Train Loss: 0.054778811 Test Loss: 0.108130414 
2019-02-20 23:19:34 - Epoch [73], Iter[42632], Time:488.456866264, learning rate : 0.000186934, Train Loss: 0.055252579 Test Loss: 0.115626528 
2019-02-20 23:27:43 - Epoch [74], Iter[43216], Time:488.466227770, learning rate : 0.000186758, Train Loss: 0.055168935 Test Loss: 0.112424017 
2019-02-20 23:35:51 - Epoch [75], Iter[43800], Time:488.355942726, learning rate : 0.000186581, Train Loss: 0.054486282 Test Loss: 0.110636194 
2019-02-20 23:44:00 - Epoch [76], Iter[44384], Time:488.451330900, learning rate : 0.000186404, Train Loss: 0.053536725 Test Loss: 0.110605017 
2019-02-20 23:52:08 - Epoch [77], Iter[44968], Time:488.459904194, learning rate : 0.000186227, Train Loss: 0.053224455 Test Loss: 0.107768722 
2019-02-21 00:00:17 - Epoch [78], Iter[45552], Time:488.541781902, learning rate : 0.000186050, Train Loss: 0.053841023 Test Loss: 0.115122463 
2019-02-21 00:08:25 - Epoch [79], Iter[46136], Time:488.470216990, learning rate : 0.000185874, Train Loss: 0.053475073 Test Loss: 0.109613621 
2019-02-21 00:16:33 - Epoch [80], Iter[46720], Time:488.416858196, learning rate : 0.000185697, Train Loss: 0.052693833 Test Loss: 0.113450075 
2019-02-21 00:24:42 - Epoch [81], Iter[47304], Time:488.493986130, learning rate : 0.000185521, Train Loss: 0.052541574 Test Loss: 0.113954281 
2019-02-21 00:32:50 - Epoch [82], Iter[47888], Time:488.417926550, learning rate : 0.000185344, Train Loss: 0.052804165 Test Loss: 0.112800487 
2019-02-21 00:40:59 - Epoch [83], Iter[48472], Time:488.429246902, learning rate : 0.000185168, Train Loss: 0.054760681 Test Loss: 0.116221225 
2019-02-21 00:49:07 - Epoch [84], Iter[49056], Time:488.464331150, learning rate : 0.000184991, Train Loss: 0.051299452 Test Loss: 0.116255336 
2019-02-21 00:57:16 - Epoch [85], Iter[49640], Time:488.406765938, learning rate : 0.000184815, Train Loss: 0.051411936 Test Loss: 0.118334596 
2019-02-21 01:05:24 - Epoch [86], Iter[50224], Time:488.441533089, learning rate : 0.000184639, Train Loss: 0.051654025 Test Loss: 0.109962960 
2019-02-21 01:13:33 - Epoch [87], Iter[50808], Time:488.398508310, learning rate : 0.000184462, Train Loss: 0.051724755 Test Loss: 0.113316757 
2019-02-21 01:21:41 - Epoch [88], Iter[51392], Time:488.524209261, learning rate : 0.000184286, Train Loss: 0.049947206 Test Loss: 0.122635639 
2019-02-21 01:29:50 - Epoch [89], Iter[51976], Time:488.424976587, learning rate : 0.000184110, Train Loss: 0.050223152 Test Loss: 0.110622009 
2019-02-21 01:37:58 - Epoch [90], Iter[52560], Time:488.481307030, learning rate : 0.000183934, Train Loss: 0.050389760 Test Loss: 0.112458899 
2019-02-21 01:46:07 - Epoch [91], Iter[53144], Time:488.491348505, learning rate : 0.000183758, Train Loss: 0.050852907 Test Loss: 0.114462233 
2019-02-21 01:54:15 - Epoch [92], Iter[53728], Time:488.424885511, learning rate : 0.000183582, Train Loss: 0.049175401 Test Loss: 0.112973578 
2019-02-21 02:02:23 - Epoch [93], Iter[54312], Time:488.514243603, learning rate : 0.000183406, Train Loss: 0.050314549 Test Loss: 0.110429141 
2019-02-21 02:10:32 - Epoch [94], Iter[54896], Time:488.454512358, learning rate : 0.000183231, Train Loss: 0.049139409 Test Loss: 0.107361642 
2019-02-21 02:18:40 - Epoch [95], Iter[55480], Time:488.446319103, learning rate : 0.000183055, Train Loss: 0.049020419 Test Loss: 0.107108179 
2019-02-21 02:26:49 - Epoch [96], Iter[56064], Time:488.436465979, learning rate : 0.000182879, Train Loss: 0.048020774 Test Loss: 0.110731756 
2019-02-21 02:34:57 - Epoch [97], Iter[56648], Time:488.569219351, learning rate : 0.000182704, Train Loss: 0.048293132 Test Loss: 0.112469488 
2019-02-21 02:43:06 - Epoch [98], Iter[57232], Time:488.449650764, learning rate : 0.000182528, Train Loss: 0.048423321 Test Loss: 0.112673506 
2019-02-21 02:51:14 - Epoch [99], Iter[57816], Time:488.523079872, learning rate : 0.000182352, Train Loss: 0.047966707 Test Loss: 0.114725877 
2019-02-21 02:59:23 - Epoch [100], Iter[58400], Time:488.393507481, learning rate : 0.000182177, Train Loss: 0.047509566 Test Loss: 0.110359626 
2019-02-21 03:07:31 - Epoch [101], Iter[58984], Time:488.461371422, learning rate : 0.000182002, Train Loss: 0.047663028 Test Loss: 0.119477842 
2019-02-21 03:15:40 - Epoch [102], Iter[59568], Time:488.468080521, learning rate : 0.000181826, Train Loss: 0.047388383 Test Loss: 0.110718023 
2019-02-21 03:23:48 - Epoch [103], Iter[60152], Time:488.492431164, learning rate : 0.000181651, Train Loss: 0.047226213 Test Loss: 0.110862176 
2019-02-21 03:31:57 - Epoch [104], Iter[60736], Time:488.497368097, learning rate : 0.000181476, Train Loss: 0.047553348 Test Loss: 0.108149931 
2019-02-21 03:40:05 - Epoch [105], Iter[61320], Time:488.679080963, learning rate : 0.000181300, Train Loss: 0.046386182 Test Loss: 0.106511950 
2019-02-21 03:48:14 - Epoch [106], Iter[61904], Time:488.485105515, learning rate : 0.000181125, Train Loss: 0.046531705 Test Loss: 0.111473012 
2019-02-21 03:56:22 - Epoch [107], Iter[62488], Time:488.375596285, learning rate : 0.000180950, Train Loss: 0.046281189 Test Loss: 0.106965180 
2019-02-21 04:04:31 - Epoch [108], Iter[63072], Time:488.352847815, learning rate : 0.000180775, Train Loss: 0.045597287 Test Loss: 0.112147837 
2019-02-21 04:12:39 - Epoch [109], Iter[63656], Time:488.459366560, learning rate : 0.000180600, Train Loss: 0.046069813 Test Loss: 0.113633613 
2019-02-21 04:20:48 - Epoch [110], Iter[64240], Time:488.414262533, learning rate : 0.000180425, Train Loss: 0.045777254 Test Loss: 0.111066655 
2019-02-21 04:28:56 - Epoch [111], Iter[64824], Time:488.406455040, learning rate : 0.000180250, Train Loss: 0.045333237 Test Loss: 0.108458594 
2019-02-21 04:37:04 - Epoch [112], Iter[65408], Time:488.426421642, learning rate : 0.000180076, Train Loss: 0.045006080 Test Loss: 0.112427490 
2019-02-21 04:45:13 - Epoch [113], Iter[65992], Time:488.422082663, learning rate : 0.000179901, Train Loss: 0.045335678 Test Loss: 0.114010815 
2019-02-21 04:53:21 - Epoch [114], Iter[66576], Time:488.485286951, learning rate : 0.000179726, Train Loss: 0.044301128 Test Loss: 0.107313803 
2019-02-21 05:01:30 - Epoch [115], Iter[67160], Time:488.580833673, learning rate : 0.000179552, Train Loss: 0.044527805 Test Loss: 0.109267346 
2019-02-21 05:09:38 - Epoch [116], Iter[67744], Time:488.362027645, learning rate : 0.000179377, Train Loss: 0.044513186 Test Loss: 0.109359547 
2019-02-21 05:17:47 - Epoch [117], Iter[68328], Time:488.471920967, learning rate : 0.000179202, Train Loss: 0.045001986 Test Loss: 0.114100711 
2019-02-21 05:25:55 - Epoch [118], Iter[68912], Time:488.543351173, learning rate : 0.000179028, Train Loss: 0.044053109 Test Loss: 0.117116111 
2019-02-21 05:34:04 - Epoch [119], Iter[69496], Time:488.519837618, learning rate : 0.000178854, Train Loss: 0.044134974 Test Loss: 0.112270658 
2019-02-21 05:42:12 - Epoch [120], Iter[70080], Time:488.602378130, learning rate : 0.000178679, Train Loss: 0.043215276 Test Loss: 0.110437254 
2019-02-21 05:50:21 - Epoch [121], Iter[70664], Time:488.702136755, learning rate : 0.000178505, Train Loss: 0.042958902 Test Loss: 0.109371801 
2019-02-21 05:58:30 - Epoch [122], Iter[71248], Time:488.404421329, learning rate : 0.000178331, Train Loss: 0.044059977 Test Loss: 0.114400541 
2019-02-21 06:06:38 - Epoch [123], Iter[71832], Time:488.493830681, learning rate : 0.000178157, Train Loss: 0.042275665 Test Loss: 0.115576653 
2019-02-21 06:14:46 - Epoch [124], Iter[72416], Time:488.391332865, learning rate : 0.000177982, Train Loss: 0.044356023 Test Loss: 0.109322761 
2019-02-21 06:22:55 - Epoch [125], Iter[73000], Time:488.426635742, learning rate : 0.000177808, Train Loss: 0.043123205 Test Loss: 0.117816115 
2019-02-21 06:31:03 - Epoch [126], Iter[73584], Time:488.445461512, learning rate : 0.000177634, Train Loss: 0.042607068 Test Loss: 0.113446574 
2019-02-21 06:39:12 - Epoch [127], Iter[74168], Time:488.415954351, learning rate : 0.000177460, Train Loss: 0.043815949 Test Loss: 0.107767642 
2019-02-21 06:47:20 - Epoch [128], Iter[74752], Time:488.384236574, learning rate : 0.000177287, Train Loss: 0.043456917 Test Loss: 0.110327878 
2019-02-21 06:55:29 - Epoch [129], Iter[75336], Time:488.467206478, learning rate : 0.000177113, Train Loss: 0.042176679 Test Loss: 0.107380924 
2019-02-21 07:03:37 - Epoch [130], Iter[75920], Time:488.469451189, learning rate : 0.000176939, Train Loss: 0.041870335 Test Loss: 0.109642111 
2019-02-21 07:11:45 - Epoch [131], Iter[76504], Time:488.390775204, learning rate : 0.000176765, Train Loss: 0.041152565 Test Loss: 0.108687438 
2019-02-21 07:19:54 - Epoch [132], Iter[77088], Time:488.513051987, learning rate : 0.000176592, Train Loss: 0.041890188 Test Loss: 0.108348818 
2019-02-21 07:28:02 - Epoch [133], Iter[77672], Time:488.431033373, learning rate : 0.000176418, Train Loss: 0.042149157 Test Loss: 0.107539698 
2019-02-21 07:36:11 - Epoch [134], Iter[78256], Time:488.551217794, learning rate : 0.000176244, Train Loss: 0.041885958 Test Loss: 0.117848790 
2019-02-21 07:44:19 - Epoch [135], Iter[78840], Time:488.452699900, learning rate : 0.000176071, Train Loss: 0.040971650 Test Loss: 0.106178172 
2019-02-21 07:52:28 - Epoch [136], Iter[79424], Time:488.449265718, learning rate : 0.000175897, Train Loss: 0.042639363 Test Loss: 0.110112490 
2019-02-21 08:00:36 - Epoch [137], Iter[80008], Time:488.480504036, learning rate : 0.000175724, Train Loss: 0.041469169 Test Loss: 0.109836324 
2019-02-21 08:08:45 - Epoch [138], Iter[80592], Time:488.434566975, learning rate : 0.000175551, Train Loss: 0.041264634 Test Loss: 0.107005891 
2019-02-21 08:16:53 - Epoch [139], Iter[81176], Time:488.464841604, learning rate : 0.000175377, Train Loss: 0.041042210 Test Loss: 0.107861247 
2019-02-21 08:25:02 - Epoch [140], Iter[81760], Time:488.456420660, learning rate : 0.000175204, Train Loss: 0.041643334 Test Loss: 0.105617553 
2019-02-21 08:33:10 - Epoch [141], Iter[82344], Time:488.425282955, learning rate : 0.000175031, Train Loss: 0.040342326 Test Loss: 0.107964647 
2019-02-21 08:41:19 - Epoch [142], Iter[82928], Time:488.448222637, learning rate : 0.000174858, Train Loss: 0.040952970 Test Loss: 0.108519005 
2019-02-21 08:49:27 - Epoch [143], Iter[83512], Time:488.456705570, learning rate : 0.000174685, Train Loss: 0.041181363 Test Loss: 0.109548888 
2019-02-21 08:57:36 - Epoch [144], Iter[84096], Time:488.452819109, learning rate : 0.000174512, Train Loss: 0.039668306 Test Loss: 0.116235636 
2019-02-21 09:05:44 - Epoch [145], Iter[84680], Time:488.446200609, learning rate : 0.000174339, Train Loss: 0.040055440 Test Loss: 0.107865650 
2019-02-21 09:13:52 - Epoch [146], Iter[85264], Time:488.459042788, learning rate : 0.000174166, Train Loss: 0.039902200 Test Loss: 0.110543368 
2019-02-21 09:22:01 - Epoch [147], Iter[85848], Time:488.415104628, learning rate : 0.000173993, Train Loss: 0.039910482 Test Loss: 0.107528287 
2019-02-21 09:30:09 - Epoch [148], Iter[86432], Time:488.443900824, learning rate : 0.000173821, Train Loss: 0.040240248 Test Loss: 0.107361995 
2019-02-21 09:38:18 - Epoch [149], Iter[87016], Time:488.421864271, learning rate : 0.000173648, Train Loss: 0.038630905 Test Loss: 0.111170987 
2019-02-21 09:46:26 - Epoch [150], Iter[87600], Time:488.395527363, learning rate : 0.000173475, Train Loss: 0.040194806 Test Loss: 0.113359561 
2019-02-21 09:54:35 - Epoch [151], Iter[88184], Time:488.421820164, learning rate : 0.000173303, Train Loss: 0.039681967 Test Loss: 0.108508462 
2019-02-21 10:02:43 - Epoch [152], Iter[88768], Time:488.445672989, learning rate : 0.000173130, Train Loss: 0.038983200 Test Loss: 0.113264093 
2019-02-21 10:10:52 - Epoch [153], Iter[89352], Time:488.556282997, learning rate : 0.000172958, Train Loss: 0.040498297 Test Loss: 0.108956344 
2019-02-21 10:19:00 - Epoch [154], Iter[89936], Time:488.452174187, learning rate : 0.000172785, Train Loss: 0.038465160 Test Loss: 0.109114231 
2019-02-21 10:27:09 - Epoch [155], Iter[90520], Time:488.532802343, learning rate : 0.000172613, Train Loss: 0.038773902 Test Loss: 0.106154103 
2019-02-21 10:35:17 - Epoch [156], Iter[91104], Time:488.593522549, learning rate : 0.000172441, Train Loss: 0.039007779 Test Loss: 0.106804371 
2019-02-21 10:43:26 - Epoch [157], Iter[91688], Time:488.453266859, learning rate : 0.000172268, Train Loss: 0.039052548 Test Loss: 0.111645714 
2019-02-21 10:51:34 - Epoch [158], Iter[92272], Time:488.677990913, learning rate : 0.000172096, Train Loss: 0.038863727 Test Loss: 0.117636190 
2019-02-21 10:59:43 - Epoch [159], Iter[92856], Time:488.621569157, learning rate : 0.000171924, Train Loss: 0.038643844 Test Loss: 0.109590029 
2019-02-21 11:07:52 - Epoch [160], Iter[93440], Time:488.526572466, learning rate : 0.000171752, Train Loss: 0.038241206 Test Loss: 0.110595651 
2019-02-21 11:16:00 - Epoch [161], Iter[94024], Time:488.489552021, learning rate : 0.000171580, Train Loss: 0.038217945 Test Loss: 0.111601343 
2019-02-21 11:24:09 - Epoch [162], Iter[94608], Time:488.572136402, learning rate : 0.000171408, Train Loss: 0.038009181 Test Loss: 0.107296056 
2019-02-21 11:32:17 - Epoch [163], Iter[95192], Time:488.507360697, learning rate : 0.000171236, Train Loss: 0.038404534 Test Loss: 0.106953711 
2019-02-21 11:40:26 - Epoch [164], Iter[95776], Time:488.728739023, learning rate : 0.000171064, Train Loss: 0.038901629 Test Loss: 0.108997126 
2019-02-21 11:48:34 - Epoch [165], Iter[96360], Time:488.488685369, learning rate : 0.000170893, Train Loss: 0.037893392 Test Loss: 0.111553273 
2019-02-21 11:56:43 - Epoch [166], Iter[96944], Time:488.438842058, learning rate : 0.000170721, Train Loss: 0.037691563 Test Loss: 0.108083256 
